{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LangChain Crash Course - Build apps with language models](https://www.youtube.com/watch?v=LbT1yp6quS8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* Installation\n",
    "* LLMs\n",
    "* Prompt Templates\n",
    "* Chains\n",
    "* Agents and Tools\n",
    "* Memory\n",
    "* Document Loaders\n",
    "* Indexes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.0.147)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.27.4)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.13.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (1.4.47)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (0.5.7)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (2.8.4)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (1.24.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (1.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (8.2.2)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in ./.venv/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 4)) (23.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 2)) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 2)) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 2)) (0.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy<2,>=1->langchain->-r requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 2)) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLMs\n",
    "\n",
    "A generic interface for all LLMS. See all LLM providers: [https://python.langchain.com/en/latest/modules/models/llms/integrations.html](https://python.langchain.com/en/latest/modules/models/llms/integrations.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Happy Socksy.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9) # model_name=\"text-davinci-003\"\n",
    "text = \"What is a good company name for a company that sells colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API: Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# https://huggingface.co/google/flan-t5-xl\u001b[39;00m\n\u001b[1;32m      6\u001b[0m llm \u001b[39m=\u001b[39m HuggingFaceHub(repo_id\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgoogle/flan-t5-xl\u001b[39m\u001b[39m\"\u001b[39m, model_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m64\u001b[39m})\n\u001b[0;32m----> 8\u001b[0m llm(\u001b[39m\"\u001b[39;49m\u001b[39mTranslate English to Spanish: How old are you?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Code/Learn/langchain/spike-langchain/.venv/lib/python3.10/site-packages/langchain/llms/base.py:246\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    245\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([prompt], stop\u001b[39m=\u001b[39;49mstop)\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n",
      "File \u001b[0;32m~/Code/Learn/langchain/spike-langchain/.venv/lib/python3.10/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Code/Learn/langchain/spike-langchain/.venv/lib/python3.10/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Code/Learn/langchain/spike-langchain/.venv/lib/python3.10/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[39m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    325\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/Code/Learn/langchain/spike-langchain/.venv/lib/python3.10/site-packages/langchain/llms/huggingface_hub.py:105\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient(inputs\u001b[39m=\u001b[39mprompt, params\u001b[39m=\u001b[39m_model_kwargs)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError raised by inference API: \u001b[39m\u001b[39m{\u001b[39;00mresponse[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     text \u001b[39m=\u001b[39m response[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mlen\u001b[39m(prompt) :]\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Internal Server Error"
     ]
    }
   ],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# https://huggingface.co/google/flan-t5-xl\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\": 0, \"max_length\": 64})\n",
    "\n",
    "llm(\"Translate English to Spanish: How old are you?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
